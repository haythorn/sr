---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# sr

<!-- badges: start -->
<!-- badges: end -->

Smooth Regression is a set of techniques for finding causal relationships in precision data.  It is built around the Gamma test.  If you have some measurements that might be predictive, and a target that you want to predict, Gamma estimates the mean squared error of the best smooth model that could be built on that data.  In linear regression, the intercept coefficient tells you the expected error of the model.  Gamma does this for smooth models as a class.  It measures smoothness in a data relationship.  Smoothness is a property of natural causal processes.  Natural processes differ from noise because they are smooth, and noise is not.  Neural networks are smooth models, as are differential equations whose derivatives don't become infinite.  Gamma is a mathematically principled answer to the overtraining problem in machine learning.

In order to detect smoothness, Gamma needs precision data.  For category and small integer data, you should use traditional methods - traditional statistics is very good at category problems.  Gamma works on continuous data.  In practice this means that values should range across two or more full digits precision in all of your variables.  .

Gamma can find causal relationships, including lags and embeddings for time series models.  It can also be used to tune the performance of neural networks. This package, 'sr',  provides the Gamma test along with a toolkit of search techniques that use it.


## Installation

You can install the development version of sr like so:

``` r
#  devtools::install_github("haythorn/sr")
```

## Example

This example shows the creation of an embedding on a time series, division of the data into training and test subsets, using `gamma_test` to control learning and prevent overtraining in a neural network, and displaying the results.  This example is explained in detail in the time_series vignette.

```{r setup_example, include=FALSE}
library(sr)
scale01 <- function(x) {
  maxx <- max(x)
  minx <- min(x)
  return(scale(x,
               center = minx,
               scale = maxx - minx))
}
hs <- scale01(henon_x)

```


```{r example, fig.width = 6, fig.height = 3.5}
library(sr)
library(nnet)
library(ggplot2)


x <- embed(as.matrix(hs), 3)   #hs is a time series

target <- x[ ,1]
train_t <- target[1:600]
test_t <-  target[601:998]

predictors <- x[ ,2:3]
train_p <- predictors[1:600, ]
test_p  <- predictors[601:998, ]

# train a neural net, using Gamma to control the training
sum_squares <- gamma_test(train_p, train_t)$Gamma * length(t)

gamma_model <- nnet(x = train_p, y = train_t, size = 8, rang = 0.1,
                decay = 1e-5, maxit = 2000, abstol =  sum_squares)


# how does the model do on test data?
predicted <- predict(gamma_model, test_p, type = "raw")
test_result <- data.frame(idx = 1:length(test_t), predicted[ ,1], test_t)
colnames(test_result) <- c("idx", "predicted", "actual")

ggplot(data = test_result[1:50, ]) +
  geom_line(mapping = aes(x = idx, y = actual, color = "green")) +
  geom_line(mapping = aes(x = idx, y = predicted, color = "blue")) +
  geom_line(mapping = aes(x = idx, y = actual - predicted, color = "red")) +
  scale_colour_manual(name = 'sequence', 
         values =c('green'='green', 'blue'='blue','red'='red'), 
         labels = c('predicted', 'actual', 'error' )) +
  labs(y = "predicted over actual",
      title = "Henon Model Using Gamma")

```

