---
title: "time_series_with_sr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{time_series_with_sr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 4, 
  fig.height = 3
)
```

```{r setup}
library(sr)
library(ggplot2)
library(nnet)
library(magrittr)
```

## Using the Gamma Test with Time Series Data

##### Smooth Regression is a set of techniques for finding causal relationships in data.  

It is built around the Gamma test, which measures smoothness in an input-output dataset.  If you have some measurements that might be predictive, and a target that you want to predict, Gamma estimates the mean squared error of the best smooth model that could be built on that data.  In linear regression, the intercept coefficient tells you the expected error of the model.  Gamma is for all smooth models.  

Smoothness is a property of natural causal processes.  Natural processes differ from noise because they are smooth, and noise is not.  Neural networks are smooth models, so are differential equations, as long as their derivatives don't become infinite.  The proof of Gamma shows that, as sampling density increases, Gamma converges on the true noise variance, relative to any differential equation with bounded derivatives.  Unlike parameterized mean squared error estimators like thE linear intercept, Gamma does not rely on any assumptions about the shape of the model, it measures whether the relationship between variables is smooth.  

In order to detect smoothness, Gamma needs precision data.  For category and small integer data, you should use traditional methods - traditional statistics is very good at category problems.  Gamma works on continuous data.  In practice this means that values should range across two or more full digits precision in all of your variables.  .

Gamma can find causal relationships, including lags and embeddings for time series models.  It can also be used to tune the performance of neural networks. This package, smooth_regression,  provides the Gamma test along with a toolkit of search techniques that use it.

This vignette will demonstrate using Gamma to 
  - search for causal relationships in time series data, 
  - determine optimum lags and embeddings, and 
  - tune the performance of a neural network.  

The vignette will use artificial data sets built from two equations that are commonly used to demonstrate chaotic dynamics.  Using artificial data will allow us to compare what the Gamma test tells us with what we know is true.




## Example 1) A chaotic function with no noise

### The Henon Map

Suppose we are asked to analyze this data:


```{r henon, fig.width = 5, fig.height=4, fig.align='center'}

  # henon_x <- read.csv("henon_ix.csv")
  str(henon_x)
  hix <- data.frame(idx = 1:length(henon_x), x = henon_x)
  p <- ggplot(data = hix) +
    geom_point(mapping = aes(x = idx, y = x), shape = ".") +
    labs(title = "Henon Map") +
         xlab("time step") +
         ylab("value")
  p

```

It looks a little more regular if we connect the dots.

```{r henonline, fig.width = 5, fig.height=4, fig.align='center'}

  p + geom_line(mapping = aes(x = idx, y = x), color = "blue")

```


Now we can see some structure to it, but it is not at all obvious that it represents a very simple pair of equations.  According to Wikipedia: "The Hénon map ... is a discrete-time dynamical system. It is one of the most studied examples of dynamical systems that exhibit chaotic behavior. The Hénon map takes a point (x[n], y[n]) in the plane and maps it to a new point

```
# x[n+1] <- 1 - a * x[n] ^ 2 + y[n]
# y[n+1] <- b * x[n]
```

For the right choices of the a and b the behavior is chaotic.  The data doesn't look smooth but the underlying functions are - their derivatives are finite.  Since y depends only on the previous x, we can predict the next x value from the two previous x values,  


### Finding a good embedding

But the overlords who are presenting us with the problem haven't told us that, maybe they don't know.  How do we figure it out?

We need answers to several questions, the first is: how far back in time do we have to look, in order to predict the next point?  Each measurement from the past has a lag.  Together the lags we look at make an embedding.  We need to find an embedding.

To do this, we begin by making a trial.  I will arbitrarily pick an embedding depth of 15, so we'll be seeing how well we can predict each point using the previous 15 - after the first 15 of course.  embed from the stats library reverses the order of the values, so the targets are in column 1, with the lag 1 predictors in column 2.


```{r embed_henon}
  search_depth <- 15       # number of candidate predictors
  henon_embedded <- embed(as.matrix(henon_x), search_depth+1)  # plus 1 target
  targets <- henon_embedded[ ,1]
  predictors <- henon_embedded[ ,2:(search_depth+1)]
  
  
```


To find the right embedding, we use an **increasing embedding search**.  This takes the lag 1 predictor and measures Gamma. Then it uses lags 1 and 2, measuring Gamma while adding an additional lag at each step.  The result is usually examined visually.  The increasing_search function returns a data frame, and plots it unless you set plot = FALSE, 

```{r plot_increasing}
  increasing_search(predictors=predictors, 
                    target=targets,
                    caption = "HenonX data")

```


Gamma estimates noise - mean squared error. This plot shows Gamma dropping to almost zero with an embedding of 2, in other words, Gamma immediately recognizes the causality in the data.  You wont get graphs like this very often with real data. Beyond an embedding of 7, gamma starts to rise again, because the extraneous variables obscure the smooth relationship. 


Looking at the graph, one might imagine that an embedding of 6 would be somehow better than 2, "more stable" or whatever.  But if the lags between 3 and 6 added any information, *Gamma would go down when they are added*.   In this case, of course, they can't add information, because the mean squared error estimate is already zero.  But in general, when you see a long flat floor on the increasing embedding search, the causality is at the beginning.  Those additional variables are not confirming votes, it's just that the relationship is strong enough that it is not obscured by a few  random variables.  

If you want to work with the numbers, you can get the result of the increasing search as a data.frame:


```{r get_increasing}
  tmp <- increasing_search(predictors=predictors, 
                    target=targets)
  tmp

```


So, looking at the graph I'm going to use an embedding of 2.  We will also scale the data, before dividing it into training and test sets for the neural network.  To build the neural net I will use the nnet package, which expects its data to be scaled between 0 and 1.  Gamma does not require scaling.


```{r gamma_test}

  scale01 <- function(x) {
    maxx <- max(x)
    minx <- min(x)
    return(scale(x, 
                 center = minx, 
                 scale = maxx - minx))
  }

  henon_scaled <- scale01(henon_x)
  
  search_depth <- 2
  henon_embedded <- embed(as.matrix(henon_scaled), search_depth+1)
  p <- henon_embedded[ ,2:(search_depth+1)]
  t <- henon_embedded[ ,1]
  gamma_test(predictors = p, target = t)
```

Real noise in the data is zero, the Gamma estimate is 1.58e-05, close enough for statistical work.  To understand the results without having to think about how the data is scaled, use the vratio, which is Gamma divided by Var(targets), so it gives the percentage of variance accounted for by estimated noise, and 1 - vratio gives the percentage accounted for by the unknown smooth function, .9998 in this case.


### Do we have enough data? - the M list

A second question is, how much confidence can we have in Gamma's estimate of the true noise?  Unfortunately, at this time there isn't any known way to calculate confidence intervals around Gamma, but the **M list** gives us a basis for a heuristic confidence estimate.  It allows us to see if Gamma becomes more stable as we add more data.  If it doesn't, we probably don't have enough data.  

The function get_Mlist computes Gamma for larger and larger portions of the data set.  What we are looking for in an M list is stability - after a certain point, Gamma should settle to an estimate that doesn't drift as more data is added.   


```{r M_test}

  get_Mlist(predictors = p, 
            target = t, 
            caption = "HenonMap.csv embedding 2")

```


In this case, as M increases, Gamma is supernaturally stable. Based on this M list, I'm going to put 600 cases in the training set and keep 400 for the test set.  


```{r}
lt <- length(t)

train_t <- t[1:600]
train_p <- p[1:600, ]

test_t <- t[601:lt]
test_p <- p[601:lt, ]

```



### Building a model

Now we are going to  build a neural net using the nnet package.  We will begin by using the parameters in the reference page example and hidden layer size equal to the number of predictors, a choice which is often recommended by the internet.


```{r nnet_model, cache=TRUE}

set.seed(3)

n_model <- nnet(x = train_p, y = train_t, size = 2, rang = 0.1,
                decay = 5e-4, maxit = 200 )

predicted_t <- predict(n_model, test_p, type = "raw")

test_result <- data.frame(idx = 1:length(test_t), predicted_t[ ,1], test_t)
colnames(test_result) <- c("idx", "predicted", "actual")


ggplot(data = test_result[1:50, ]) +
  geom_line(mapping = aes(x = idx, y = actual), color = "green") +
  geom_line(mapping = aes(x = idx, y = predicted), color = "blue") +
  geom_line(mapping = aes(x = idx, y = actual - predicted), color = "red") +
  labs(y = "predicted over actual",
      title = "Henon Model Test")

# Gamma estimate of noise sse
train_gamma <- gamma_test(predictors = train_p, target = train_t)$Gamma
train_gamma * length(train_t)

# sum of squared error according to model residuals
sum(n_model$residuals ^ 2)

# sum of squared errors on test data
with(test_result, sum((actual - predicted)^2))


```


For a quick model built with the default parameters, this looks pretty good, but according to Gamma we should be able to to a lot better.  The model residuals are .64 and actual error on test data is .50, where Gamma says we should be getting .014.  

This vignette is not going to go into detail about working between Gamma and neural net software, that deserves a vignette of its own, but we will make one pass at it here, to show you an idea of the general approach.  The Gamma test can be used with any neural net software that allows a mean squared error stopping condition for training.  nnet allows this with the abstol parameter. We will build a new net using this stopping condition, more hidden layer units, and a smaller decay parameter.  Decay is used to prevent overtraining, we have the Gamma test which is a more principled way to do that.  In general, when you adapt a model to Gamma, you will want to clear away a lot of stuff that's called "regularization", much of it is kludges that people have used because they didn't have a good mean squared error estimator.


```{r nnet_by_gamma, cache=TRUE}

estimated_sse <- train_gamma * length(train_t)
gamma_model <- nnet(x = train_p, y = train_t, size = 8, rang = 0.1,
                decay = 1e-5, maxit = 2000, abstol =  estimated_sse)

predicted_gt <- predict(gamma_model, test_p, type = "raw")

test_result <- data.frame(idx = 1:length(test_t), predicted_gt[ ,1], test_t)
colnames(test_result) <- c("idx", "predicted", "actual")

ggplot(data = test_result[1:50, ]) +
  geom_line(mapping = aes(x = idx, y = actual), color = "green") +
  geom_line(mapping = aes(x = idx, y = predicted), color = "blue") +
  geom_line(mapping = aes(x = idx, y = actual - predicted), color = "red") +
  labs(y = "predicted over actual",
      title = "Henon Model Using Gamma")

# Gamma estimate of error -  gamma times number of observations
estimated_sse  

# error according to model residuals
sum(gamma_model$residuals ^ 2)

# sum of squared errors on test data
with(test_result, sum((actual - predicted)^2))



```

This does considerably better.  Gamma still says we can cut the error in half but I'm not going to fuss around with it.  Neural net building is a topic for another vignette, using neural net software that has some documentation and more than one hidden layer. 


### Summary: 


**This first example has shown the Gamma test used in four ways:**

  - **to identify an embedding** - How many previous values of the time series do we want, in order to predict the next one?  We saw the Gamma test immediately recognize a causal function even though it was chaotic, and identify the correct embedding using a simple increasing search.
  
  - **to decide how much data is needed to build a model** - The M list tells us how much confidence we can have in Gamma's estimate.  It allows us to make a more informed decision about whether we have enough data, and how much data to use for model training vs. testing.
  
  - **to evaluate the performance of a model** - Although our first model looked OK, given the complexity of what it had to model, the Gamma test told us that we could do better.  A mean squared error estimator tells you how well your model should do, so you know when you need to build another model.  
  
  - **to prevent overtraining** - Learning algorithms are subject to overfitting.  Gamma does not work by minimizing some parameter, it directly measures the variance of the noise.  It is called the Gamma "test" because it directly tests for the presence of a smooth function.  This makes it a good complement to neural net learning algorithms and local linear regression.  
  


## Example 2) A complex function obscured by noise


The first example showed that the Gamma test can recognize a causal relationship in data.  The second example will show a Gamma analysis under more difficult conditions.  This is also an artificial data set like the first, but the function is more complex and it will be obscured by noise.   We will analyze the data using the tools of smooth regression, and then look at how it was constructed.  


### Analysing Noisy Data


Here's our time series:


```{r example2_data}

str(example2_data)
data.frame(idx = 1:length(example2_data), x = example2_data) %>%
  ggplot(data = .) +
      geom_line(mapping = aes(x = idx, y = x), color = "blue") +
      labs(title = "example2 2 data") +
           xlab("time step") +
           ylab("value")


```



Begin with the increasing embedding search


```{r i_search, fig.width = 5, fig.height=4, fig.align='center'}

i_search <- function(data, depth) {
  x <- embed(as.matrix(data), depth+1)
  p <- x[ ,2:(depth+1)]
  t <- x[ ,1]

  increasing_search(p, t)
}

search_depth <- 12    # trial embedding
i_search(example2_data, search_depth)
```

The first four lags clearly have a strong effect, and then the graph seems to level off but may be still falling.  Let's get an M list, embedding on the first 4 lags:


```{r mgls_Mtest, fig.width = 5, fig.height=4, fig.align='center'}
embed4 <- embed(as.matrix(example2_data), 5)
  p <- embed4[ ,2:5]
  t <- embed4[ ,1]

  get_Mlist(predictors = p, target = t, by = 50) 
  
```


This might be converging but doesn't give me confidence.  Let's look deeper.  

I want you to examine the next graph carefully.  Low Gamma indicates a relationship, but Gamma is an estimator working on a particular data sample, so just looking for the lowest Gamma doesn't work.  You're looking for patterns in the behavior of Gamma.  What do you see?


```{r longer_search, fig.width = 5, fig.height=4, fig.align='center'}

i_search(example2_data, 60)

```

As I read this, there is a lot of information in the first 4 lags, and then a small steady amount of information with each additional lag out to around 25.  Sharp decreases in 1 to 4, followed by a slow linear decrease across 5 through 24.  After that the graph is more volatile and doesn't show any clear trend.  I'm not at all sure of this interpretation, but decreasing Gamma across a range of lags like we see here is worth looking into.  Fortunately, we have a way to look into it - get an M list:

```{r mgls32, fig.width = 5, fig.height=4, fig.align='center'}
  embed32 <- embed(as.matrix(example2_data), 33)
  p32 <- embed32[ ,2:33]
  t32 <- embed32[ ,1]

  get_Mlist(predictors = p32, target = t32, by = 50, caption = "32 lag embedding")
  
 
```


```{r another_mgls_Mtest, fig.width = 5, fig.height=4, fig.align='center'}
  embed24 <- embed(as.matrix(example2_data), 25)
  p24 <- embed24[ ,2:25]
  t24 <- embed24[ ,1]

  get_Mlist(predictors = p24, target = t24, by = 50, caption = "24 lag embedding")
  
 
```


This is better.  This graph gives me confidence that there is something here. The proof of the Gamma test says that it will converge on the true noise in the data sample, as sampling density on the function increases.  We use the M list to look for this convergence.  Here, Gamma is stable across half of the data set, showing a vratio of 19%.



### How this data was created

In fact the noise was set to be 20% of the sample variance. The signal in this data is the Mackey-Glass time delayed differential equation, a chaotic time series that is used to model biological systems.  



```{r mgls}

str(mgls)

data.frame(idx = 1:length(mgls), x = mgls) %>%
  ggplot(data = .) +
    geom_point(mapping = aes(x = idx, y = x),
               shape = ".") +
    labs(title = "mgls raw data")

```

This is what chaos looks like.  The regular edges show the action of the attractor, but of course they are only clues.  We get an increasing search and an M list:

```{r}

me <- embed(as.matrix(mgls), 13)
pr <- me[ ,2:13]
tr <- me[ ,1]

```



---
  output: html_document
---

### Mackey-Glass without noise


:::: {style="display: flex;"}

::: {}

```{r}
  increasing_search(predictors=pr, target=tr)

```


:::

::: {}


```{r}
  get_Mlist(predictors = pr, target = tr, by = 100)

```

:::

::::

This is definitive.  The function is in fact driven by the first four lags, and Gamma has shown again that it can reliably detect smooth predictive relationships in data.  

Continuing, we add the noise.


```{r mgls_add_noise}

# set proportion of variance that will be noise
noise_to_signal <- .20          # to get this
add_this <- 1 / (1 / noise_to_signal - 1)
add_this    

# generate noise
vraw <- var(mgls)
n <- rnorm(length(mgls), 
           mean = mean(mgls), 
           sd = sqrt(vraw * add_this))

# add noise, check it out
mgls_noisy <- mgls + n

vraw
var(n)
var(mgls_noisy)
```


One question that arises is whether scaling affects Gamma.  We perform a Gamma test before and after the scaling:


```{r mgls_scaling}
# now scale, test Gamma before and after
x <- embed(as.matrix(mgls_noisy), 13)
gamma_test(x[ ,2:13], x[ ,1])

mgls_scaled <- scale01(mgls_noisy)

x <- embed(as.matrix(mgls_scaled), 13)
gamma_test(x[ ,2:13], x[ ,1])

```


Notice that the vratio does not change with the scaling, so this scaling does not affect Gamma's estimate of percent noise in the data.

Looking at the model without noise, we see that each point can be predicted from the four previous points, but under conditions of noise, a better model can be found as a function of many more inputs.  One of the consequences of the Gamma test mathematics is a proof that when there is noise on the inputs, the best predictive model is generally not the true model.  Antonia Jones, the principal author of the Gamma test, said this had "disturbing implications for our understanding of the world".  In this case, if the lag 4 point is obscured by noise, but it's value is strongly influenced by lag 5, then knowing lag 5 helps us predict the next point by an indirect connection.  With the Mackey-Glass function, successive points are so interdependent that by using 24 previous points, we were able to capture almost all of the information in the data.

Write the noise to the example file.  Notice that the example file you got was the one that ran the last time, if you change the noise_to_signal you won't see it until the next run.


```{r write_example2, eval = FALSE}
save(mgls_scaled, file = "example2_data.Rmd")
```




## Example 3: Just plain noise


What happens if we apply Gamma to normally distributed random noise?

```{r noise_only}
noise <- rnorm(5000)

e_noise <- embed(noise, 15+1)
n_target <- e_noise[ , 1]
n_predictors <- e_noise[ , 2:(15+1) ]

```

---
  output: html_document
---

### Random noise


:::: {style="display: flex;"}

::: {}


```{r noise_is}
increasing_search(n_predictors, 
                  n_target, 
                  caption = "random variable")

```

:::

::: {}

```{r noise_Ml}
  get_Mlist(predictors = n_predictors, 
            target = n_target, 
            by = 100, 
            caption = "randomw variable")
```

:::

::::

All the embeddings show Gamma close to 1, and this M list seems to be converging there.


## Appendix: Looking under the hood at Gamma

The gamma_test function has a plot option that allows us to see how the near neighbor search relates to the computation of Gamma.  We will look at the noise cases first.


### The pure noise case:

```{r pure_noise_plot}
gamma_test(n_predictors, n_target, plot = TRUE, caption = "random variable")
```

The black dots are the distances in output space (y) plotted against distances to near neighbors in the input space (x) .  The red dots are the average distances for the first, second, third and so on nearest neighbors.  The Gamma regression line in blue is drawn using the squares of those averages.  As data density increases, the y intercept of the gamma regression line converges on the noise variance.

In this plot we see pure noise.  The distances to near neighbors in output space have no relation to the distance in input space.  The Gamma regression line is horizontal, the y intercept is at 1, and the vratio is 1.  Gamma is telling us, correctly, that all of the variance in this data comes from noise, the predictors have no smooth relationship with the target. 


### Mixed causality and noise:

```{r mixed_plot}
gamma_test(predictors=p24, target=t24, plot = TRUE, caption = "mixed causality and noise")
```
  
This plot shows causality with noise.  In real world data the noise would be caused by measurement error or causal variables that are not being measured.  In the plot, the Gamma regression line has positive slope and the y intercept is well below 1, indicating the presence of a smooth relationship, The intercept is above 0, indicating the presence of noise.  The vration is 0.19, so estimated noise is 19% of the sample variance.


### Henon with no noise:

```{r no_noise_plot}
  dim <- ncol(henon_embedded)
  p <- henon_embedded[ ,2:dim]
  t <- henon_embedded[ ,1]
  gamma_test(predictors = p, target = t, plot = TRUE, caption = "Deterministic function")

```

The gamma regression line has positive slope and its intercept is zero, showing that the inputs and output are related by an unknown smooth function.  This triangular shape of the delta-gamma graph shows that smooth function.  If this were a linear function, change in delta would be linearly related to change in gamma, so all of the black points would lie on the gamma regression line.  Instead, because the function has curvature, points lie above and below the regression line.  Between a point and its near neighbors, either the deltas or gammas change more rapidly, the curve separates from the tangent, producing the spray of points seen here. 

